import os
import sys
import zipfile
import re
import json
import logging
import time
from pathlib import Path
from collections import defaultdict
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity as tfidf_cosine_similarity

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from utils.config import INTERVIEW_DATA_DIR, PROCESSED_DATA_DIR, LOG_DIR
from utils.constants import NAMES, CATEGORIES

def extract_time(title):
    match = re.search(r"\d{4}", title)
    return match.group() if match else "æœªçŸ¥"

def extract_participants(content):
    for name in NAMES:
        if name in content:
            return name
    return "æœªçŸ¥"

def extract_theme(title, content):
    for catg in CATEGORIES:
        if catg in title or catg in content:
            return catg
        else:
            return "å¸¸è§„è®¿è°ˆ"

def is_valid_txt(name):
    return (
        name.endswith(".txt")
        and "__MACOSX" not in name
        and not os.path.basename(name).startswith("._")
    )

def normalize_text(text):
    text = re.sub(r"[Qï¼¡]?[.\dï¼š:]+", "", text)
    text = text.replace("\n", "").replace(" ", "")
    text = text.replace("â€œ", "\"").replace("â€", "\"").replace("â€˜", "'").replace("â€™", "'")
    return text.strip()

def extract_all_texts(data_dir):
    all_entries = []
    for root, _, files in os.walk(data_dir):
        for file in files:
            path = os.path.join(root, file)
            if "__MACOSX" in path or file.startswith("._"):
                continue
            if file.endswith(".txt"):
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        text = f.read().strip()
                    filename = os.path.basename(path).replace(".txt", "")
                    all_entries.append({"text": text, "source": path, "title": filename})
                except Exception as e:
                    logging.warning(f"âŒ æœ¬åœ° txt è§£ç å¤±è´¥: {path} åŸå› : {e}")
            elif file.endswith(".zip"):
                try:
                    with zipfile.ZipFile(path, "r") as zipf:
                        for name in zipf.namelist():
                            if "__MACOSX" in name or os.path.basename(name).startswith("._"):
                                continue
                            if name.endswith(".txt"):
                                try:
                                    with zipf.open(name) as f:
                                        text = f.read().decode("utf-8").strip()
                                    filename = os.path.basename(name).replace(".txt", "")
                                    all_entries.append({"text": text, "source": f"{path}:{name}", "title": filename})
                                except Exception as e:
                                    logging.warning(f"âŒ zip è§£ç å¤±è´¥: {path} ä¸­çš„ {name}ï¼ŒåŸå› : {e}")
                except Exception as e:
                    logging.error(f"âŒ æ— æ³•æ‰“å¼€ zip æ–‡ä»¶: {path}ï¼ŒåŸå› : {e}")
    return all_entries

def cluster_texts(entries, sentence_threshold=0.9, min_match_count=10):
    """
    å¿«é€ŸåŸºäºå±€éƒ¨æ–‡æœ¬ç›¸ä¼¼åº¦çš„è®¿è°ˆèšç±»ã€‚
    è‹¥ä¸¤ç¯‡æ–‡ç« æœ‰ >= min_match_count æ¡å¥å­å½¼æ­¤ç›¸ä¼¼ï¼ˆcos > sentence_thresholdï¼‰ï¼Œåˆ™è®¤ä¸ºå¯ä»¥åˆå¹¶ã€‚
    """
    from sklearn.utils import murmurhash3_32
    
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # æå–å¥å­
    all_sentences = []
    entry_sent_idx = []  # entry_id -> list of sentence indices

    for idx, entry in enumerate(entries):
        sents = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', entry['text'])
        sents = [s.strip() for s in sents if len(s.strip()) > 6]  # å»é™¤çŸ­å¥
        all_sentences.extend(sents)
        entry_sent_idx.append((idx, list(range(len(all_sentences) - len(sents), len(all_sentences)))))

    # å‘é‡åŒ–æ‰€æœ‰å¥å­ï¼ˆä¸€æ¬¡æ€§ï¼‰
    sentence_embeddings = model.encode(all_sentences, show_progress_bar=True)

    # ä¸ºæ¯ç¯‡æ–‡ç« å»ºç«‹ MinHash ç­¾åé›†åˆ
    sig_sets = []
    for _, sent_ids in entry_sent_idx:
        hashes = set(murmurhash3_32(str(sentence_embeddings[i]), positive=True) for i in sent_ids)
        sig_sets.append(hashes)

    # ç®€å•åˆ¤é‡ï¼šè‹¥ hash é‡å æ•° >= Nï¼Œåˆ™åˆ¤ä¸ºè¿‘ä¼¼
    clusters = []
    used = set()
    for i in range(len(entries)):
        if i in used:
            continue
        cluster = [i]
        used.add(i)
        for j in range(i + 1, len(entries)):
            if j in used:
                continue
            overlap = len(sig_sets[i] & sig_sets[j])
            if overlap >= min_match_count:
                cluster.append(j)
                used.add(j)
        clusters.append(cluster)
        
    # åå¤„ç†ï¼šå°è¯•é¿å…å°†æŸäº›ç‰¹æ®Šæ–‡æœ¬ï¼ˆå¦‚ bbs_aptxï¼‰å•ç‹¬ç•™åœ¨ cluster ä¸­
    final_clusters = []
    for cluster in clusters:
        if len(cluster) > 1:
            final_clusters.append(cluster)
        else:
            i = cluster[0]
            if "bbs_aptx.txt" in entries[i]["source"]:
                # å°è¯•åŠ å…¥åˆ°æœ€ç›¸è¿‘çš„å·²æœ‰ cluster ä¸­
                max_overlap = 0
                best_cluster = None
                for c in final_clusters:
                    j = c[0]  # ä»»å–ä»£è¡¨é¡¹
                    overlap = len(sig_sets[i] & sig_sets[j])
                    if overlap > max_overlap:
                        max_overlap = overlap
                        best_cluster = c
                if best_cluster:
                    best_cluster.append(i)
                else:
                    logging.warning(f"ğŸŸ¡ {entries[i]['source']} æ— æ³•ä¸å…¶ä»–è®¿è°ˆåŒ¹é…ï¼Œå·²å¿½ç•¥")
            else:
                final_clusters.append(cluster)
    
    return final_clusters

def merge_clusters(entries, clusters):
    merged = []
    for idx, cluster in enumerate(clusters):
        merged_texts = [entries[i]["text"] for i in cluster]
        sources = [entries[i]["source"] for i in cluster]
        representative_idx = next(i for i in cluster if "bbs_aptx" not in entries[i]["title"])
        representative_text = entries[representative_idx]["text"]
        raw_title = entries[representative_idx].get("title", "").strip()
        title = raw_title if raw_title else f"è‡ªåŠ¨ç”Ÿæˆè®¿è°ˆ_{idx+1}"
        metadata = {
            "time": extract_time(title),
            "theme": extract_theme(title, representative_text),
            "participants": extract_participants(representative_text)
        }
        merged.append({
            "id": f"interview_{idx+1}",
            "title": title,
            "content": representative_text,
            "sources": sources,
            "time": metadata["time"],
            "theme": metadata["theme"],
            "participants": metadata["participants"]
        })
    return merged

os.makedirs(LOG_DIR, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join(LOG_DIR, "merge_log.txt"), mode="a", encoding="utf-8")
    ]
)

def main():
    raw_dir = INTERVIEW_DATA_DIR
    output_path = os.path.join(PROCESSED_DATA_DIR, "merged_interviews.json")
    Path(PROCESSED_DATA_DIR).mkdir(parents=True, exist_ok=True)
    start_time = time.time()
    logging.info("ğŸ“‚ æå–æ‰€æœ‰æ–‡æœ¬ä¸­...")
    all_entries = extract_all_texts(raw_dir)
    logging.info(f"âœ… å…±æå–æ–‡æœ¬æ•°é‡: {len(all_entries)}")
    logging.info("ğŸ” è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¹¶èšç±»...")
    clusters = cluster_texts(all_entries)
    logging.info(f"âœ… èšç±»å®Œæˆï¼Œç”Ÿæˆè®¿è°ˆæ¡æ•°: {len(clusters)}")
    logging.info("ğŸ§© åˆå¹¶èšç±»å†…å®¹...")
    merged = merge_clusters(all_entries, clusters)
    logging.info(f"ğŸ’¾ ä¿å­˜åˆ° {output_path}")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(merged, f, ensure_ascii=False, indent=2)
    duration = time.time() - start_time
    logging.info(f"ğŸ‰ å®Œæˆï¼å…±å†™å…¥ {len(merged)} æ¡è®¿è°ˆï¼Œç”¨æ—¶ {duration:.2f} ç§’")

if __name__ == "__main__":
    main()
