"""
LLM æœåŠ¡æ¨¡å— - æ”¯æŒå¤šä¸ª LLM æä¾›å•†çš„æŠ½è±¡æ¥å£
ç›®å‰æ”¯æŒï¼šGemini 2.0 Flash
"""

import os
import json
from typing import Dict, Any, Optional, List
from abc import ABC, abstractmethod


class LLMProvider(ABC):
    """LLM æä¾›å•†æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def extract_keywords(self, question: str) -> Dict[str, Any]:
        """  
        ä»ç”¨æˆ·é—®é¢˜ä¸­æå–å…³é”®è¯
        
        Args:
            question: ç”¨æˆ·æé—®
            
        Returns:
            åŒ…å«å…³é”®è¯çš„å­—å…¸ï¼Œæ ¼å¼ï¼š
            {
                "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", ...],
                "question_type": "æ¼«ç”»å†…å®¹" | "è®¿è°ˆä¿¡æ¯" | "ç»¼åˆæŸ¥è¯¢",
                "confidence": 0.8
            }
        """
        pass
    
    @abstractmethod
    def generate_answer(self, question: str, context_texts: List[str], sources: List[str]) -> str:
        """
        æ ¹æ®ä¸Šä¸‹æ–‡æ–‡æœ¬ç”Ÿæˆå›ç­”
        
        Args:
            question: ç”¨æˆ·åŸå§‹é—®é¢˜
            context_texts: ç›¸å…³æ–‡æœ¬åˆ—è¡¨
            sources: å¯¹åº”çš„æ¥æºåˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„ç®€æ´å›ç­”
        """
        pass


class GeminiProvider(LLMProvider):
    """Gemini LLM æä¾›å•†å®ç°"""
    
    def __init__(self):
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯"""
        try:
            from google import genai
            # API key ä»ç¯å¢ƒå˜é‡ GEMINI_API_KEY è·å–
            self.client = genai.Client()
        except ImportError:
            raise ImportError("è¯·å®‰è£… google-genai åº“ï¼špip install google-genai")
        except Exception as e:
            raise Exception(f"Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
    
    def extract_keywords(self, question: str) -> Dict[str, Any]:
        """ä½¿ç”¨ Gemini 2.0 Flash æå–å…³é”®è¯"""
        if not self.client:
            raise Exception("Gemini å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“é—¨åˆ†æåä¾¦æ¢æŸ¯å—ç›¸å…³é—®é¢˜çš„åŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œæå–å…³é”®è¯å¥å¹¶åˆ¤æ–­é—®é¢˜ç±»å‹ã€‚

ç”¨æˆ·é—®é¢˜ï¼š"{}"

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
1. keywords: å…³é”®è¯åˆ—è¡¨ï¼ˆæ—¥æ–‡å’Œä¸­æ–‡éƒ½è¦è€ƒè™‘ï¼Œä¼˜å…ˆæå–å¯èƒ½åœ¨æ¼«ç”»åŸæ–‡å’Œé‡‡è®¿ä½œè€…é’å±±åˆšæ˜Œæ—¶ä¼šå‡ºç°çš„è¯æ±‡, 
å¦‚æœè¯æ±‡è¿‡äºå¸¸è§æ¯”å¦‚: "æŸ¯å—", "ç°åŸå“€" ç­‰ï¼Œåˆ™è¿”å›æ›´åŠ æœ‰æ„ä¹‰çš„çŸ­å¥æ¯”å¦‚: "å·¥è—¤æ–°ä¸€çš„çœŸå®èº«ä»½" â€œç°åŸå“€å–œæ¬¢çš„å“ç‰Œâ€ï¼‰
2. question_type: é—®é¢˜ç±»å‹ï¼Œé€‰æ‹©ä¸€ä¸ªï¼š
   - "æ¼«ç”»å†…å®¹": è¯¢é—®æ¼«ç”»å‰§æƒ…ã€è§’è‰²å¯¹è¯ã€å…·ä½“åœºæ™¯ç­‰
   - "è®¿è°ˆä¿¡æ¯": è¯¢é—®ä½œè€…è®¿è°ˆã€å¹•ååˆ¶ä½œã€è§’è‰²è®¾å®šç­‰  
   - "ç»¼åˆæŸ¥è¯¢": éœ€è¦ç»¼åˆå¤šç§èµ„æ–™æ¥å›ç­”
3. confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´çš„æ•°å€¼ï¼‰

ç¤ºä¾‹æ ¼å¼ï¼š
{{
  "keywords": ["æŸ¯å—çš„çœŸå®èº«ä»½", "å·¥è—¤æ–°ä¸€"],
  "question_type": "æ¼«ç”»å†…å®¹", 
  "confidence": 0.9
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Š!!!""".format(question)

        result_text = ""        
        try:
            print("ğŸ”„ æ­£åœ¨è°ƒç”¨ Gemini API...")
            response = self.client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )
            
            # è§£æ JSON å“åº”
            result_text = response.text
            if result_text is None:
                result_text = ""
            result_text = result_text.strip()
            
            print(f"ğŸ“¥ API åŸå§‹å“åº”: {result_text[:200]}...")
            
            # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
            if result_text.startswith("```json"):
                result_text = result_text[7:]
            if result_text.endswith("```"):
                result_text = result_text[:-3]
            
            result = json.loads(result_text.strip())
            
            # éªŒè¯ç»“æœæ ¼å¼
            if not all(key in result for key in ["keywords", "question_type", "confidence"]):
                raise ValueError("è¿”å›æ ¼å¼ä¸å®Œæ•´")
            
            print("âœ… API è°ƒç”¨æˆåŠŸ")
            return result
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON è§£æå¤±è´¥ï¼š{e}")
            # å°è¯•è·å–åŸå§‹å“åº”æ–‡æœ¬
            try:
                print(f"åŸå§‹å“åº”: {result_text}")
            except:
                print("æ— æ³•è·å–åŸå§‹å“åº”")
            # è¿”å›é»˜è®¤ç»“æœ
            return {
                "keywords": [question],  # å°†æ•´ä¸ªé—®é¢˜ä½œä¸ºå…³é”®è¯
                "question_type": "ç»¼åˆæŸ¥è¯¢",
                "confidence": 0.5
            }
        except Exception as e:
            print(f"âŒ Gemini API è°ƒç”¨å¤±è´¥ï¼š{e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return {
                "keywords": [question],
                "question_type": "ç»¼åˆæŸ¥è¯¢", 
                "confidence": 0.3
            }
    
    def generate_answer(self, question: str, context_texts: List[str], sources: List[str]) -> str:
        """ä½¿ç”¨ Gemini 2.5 Flash ç”Ÿæˆå›ç­”"""
        if not self.client:
            raise Exception("Gemini å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        if not context_texts:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = []
        for i, (text, source) in enumerate(zip(context_texts, sources), 1):
            context_info.append(f"ã€æ¥æº{i}ï¼š{source}ã€‘\n{text}")
        
        context_str = "\n\n".join(context_info)
        
        prompt = f"""ä½ æ˜¯åä¾¦æ¢æŸ¯å—çš„ä¸“ä¸šé—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„è®¿è°ˆèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

ç›¸å…³è®¿è°ˆèµ„æ–™ï¼š
{context_str}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„èµ„æ–™è¿›è¡Œå›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œ3-5å¥è¯å³å¯
3. å¦‚æœèµ„æ–™ä¸­æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œç›´æ¥å¼•ç”¨ï¼›å¦‚æœæ²¡æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œè¯´æ˜èµ„æ–™ä¸­çš„ç›¸å…³ä¿¡æ¯
4. åœ¨å›ç­”ç»“å°¾ç®€å•æåŠä¿¡æ¯æ¥æºï¼ˆå¦‚ï¼šæ ¹æ®ä½œè€…è®¿è°ˆã€æ ¹æ®å®˜æ–¹èµ„æ–™ç­‰ï¼‰
5. ä½¿ç”¨ä¸­æ–‡å›ç­”

è¯·ç›´æ¥ç»™å‡ºå›ç­”ï¼Œä¸è¦æœ‰é¢å¤–çš„è§£é‡Šæˆ–æ ¼å¼ï¼š"""

        try:
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆ AI å›ç­”...")
            response = self.client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt
            )
            
            answer = response.text
            if answer is None:
                answer = ""
            answer = answer.strip()
            
            print("âœ… AI å›ç­”ç”ŸæˆæˆåŠŸ")
            return answer if answer else "æŠ±æ­‰ï¼Œæ— æ³•åŸºäºå½“å‰èµ„æ–™ç”Ÿæˆå›ç­”ã€‚"
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥ï¼š{e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"


class MockProvider(LLMProvider):
    """æ¨¡æ‹Ÿ LLM æä¾›å•†ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    
    def extract_keywords(self, question: str) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå…³é”®è¯æå–"""
        # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘ï¼ˆç”¨äºå¼€å‘æµ‹è¯•ï¼‰
        keywords = []
        
        # å¸¸è§è§’è‰²åç§°
        characters = ["æŸ¯å—", "æ–°ä¸€", "å°å…°", "å°å“€", "ç°åŸ", "åšå£«", "åŸºå¾·", "èµ¤äº•", "å®‰å®¤"]
        for char in characters:
            if char in question:
                keywords.append(char)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è§’è‰²ï¼Œä½¿ç”¨æ•´ä¸ªé—®é¢˜
        if not keywords:
            keywords = [question]
        
        return {
            "keywords": keywords,
            "question_type": "ç»¼åˆæŸ¥è¯¢",
            "confidence": 0.6
        }
    
    def generate_answer(self, question: str, context_texts: List[str], sources: List[str]) -> str:
        """æ¨¡æ‹Ÿå›ç­”ç”Ÿæˆ"""
        if not context_texts:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
        
        # ç®€å•çš„æ¨¡æ‹Ÿå›ç­”é€»è¾‘
        first_context = context_texts[0][:100] + "..." if len(context_texts[0]) > 100 else context_texts[0]
        
        return f"æ ¹æ®è®¿è°ˆèµ„æ–™ï¼Œ{first_context} è¿™æ˜¯åŸºäº {sources[0] if sources else 'ç›¸å…³èµ„æ–™'} çš„ä¿¡æ¯ã€‚ï¼ˆè¿™æ˜¯æ¨¡æ‹Ÿå›ç­”ï¼Œå®é™…ä½¿ç”¨è¯·é…ç½® Gemini APIï¼‰"


class LLMService:
    """LLM æœåŠ¡ç®¡ç†å™¨"""
    
    def __init__(self, provider_type: str = "gemini"):
        """
        åˆå§‹åŒ– LLM æœåŠ¡
        
        Args:
            provider_type: æä¾›å•†ç±»å‹ ("gemini", "mock")
        """
        self.provider_type = provider_type
        self.provider = self._create_provider()
    
    def _create_provider(self) -> LLMProvider:
        """åˆ›å»º LLM æä¾›å•†å®ä¾‹"""
        if self.provider_type == "gemini":
            return GeminiProvider()
        elif self.provider_type == "mock":
            return MockProvider()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†ç±»å‹ï¼š{self.provider_type}")
    
    def extract_keywords(self, question: str) -> Dict[str, Any]:
        """æå–å…³é”®è¯ï¼ˆç»Ÿä¸€æ¥å£ï¼‰"""
        return self.provider.extract_keywords(question)
    
    def generate_answer(self, question: str, context_texts: List[str], sources: List[str]) -> str:
        """ç”Ÿæˆå›ç­”ï¼ˆç»Ÿä¸€æ¥å£ï¼‰"""
        return self.provider.generate_answer(question, context_texts, sources)


# å·¥å‚å‡½æ•°
def create_llm_service(provider_type: Optional[str] = None) -> LLMService:
    """
    åˆ›å»º LLM æœåŠ¡å®ä¾‹
    
    Args:
        provider_type: æä¾›å•†ç±»å‹ï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
    """
    if provider_type is None:
        provider_type = os.environ.get("LLM_PROVIDER", "gemini")
    
    return LLMService(provider_type)