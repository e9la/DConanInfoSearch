"""
å†…å®¹æœç´¢æœåŠ¡æ¨¡å— - åŸºäºå…³é”®è¯çš„æ™ºèƒ½è®¿è°ˆå†…å®¹æœç´¢
å®ç°ç›¸å…³æ€§è¯„åˆ†ã€ä¸Šä¸‹æ–‡æå–ã€å»é‡å’Œé•¿åº¦æ§åˆ¶
"""

import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from collections import defaultdict

from utils.interview_sources import get_interview_metadata


@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç±»"""
    text: str                      # ä¸Šä¸‹æ–‡æ–‡æœ¬
    keywords_found: List[str]      # åŒ¹é…çš„å…³é”®è¯
    relevance_score: float         # ç›¸å…³æ€§åˆ†æ•°
    source: str                    # æ¥æºæ–‡ä»¶
    url: Optional[str]             # åŸæ–‡é“¾æ¥  
    highlighted_text: str          # é«˜äº®å…³é”®è¯åçš„æ–‡æœ¬
    start_position: int            # åœ¨åŸæ–‡ä¸­çš„èµ·å§‹ä½ç½®
    end_position: int              # åœ¨åŸæ–‡ä¸­çš„ç»“æŸä½ç½®


class ContentSearchService:
    """å†…å®¹æœç´¢æœåŠ¡"""
    
    def __init__(self, interview_cache: Dict[str, str]):
        """
        åˆå§‹åŒ–æœç´¢æœåŠ¡
        
        Args:
            interview_cache: è®¿è°ˆæ–‡æœ¬ç¼“å­˜å­—å…¸
        """
        self.interview_cache = interview_cache
        self.sentence_delimiters = r'[ã€‚ï¼ï¼Ÿ\n]'
        
    def search_keywords(self, keywords: List[str], max_length: int = 10000) -> List[SearchResult]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢ç›¸å…³å†…å®¹
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            max_length: ç»“æœæ€»é•¿åº¦é™åˆ¶
            
        Returns:
            æŒ‰ç›¸å…³æ€§æ’åºçš„æœç´¢ç»“æœåˆ—è¡¨
        """
        if not keywords:
            return []
        
        print(f"ğŸ” å¼€å§‹æœç´¢å…³é”®è¯: {keywords}")
        
        # 1. æ”¶é›†æ‰€æœ‰åŒ¹é…ç»“æœ
        all_matches = self._find_all_matches(keywords)
        print(f"ğŸ“Š æ‰¾åˆ° {len(all_matches)} ä¸ªåˆæ­¥åŒ¹é…")
        
        # 2. æå–ä¸Šä¸‹æ–‡å¹¶è¯„åˆ†
        search_results = []
        for match in all_matches:
            # ä¼ é€’æ‰€æœ‰å…³é”®è¯ç”¨äºä¸Šä¸‹æ–‡åˆ†æ
            match['all_keywords'] = keywords
            context = self._extract_context(match)
            if context:
                search_results.append(context)
        
        print(f"ğŸ“ æå– {len(search_results)} ä¸ªä¸Šä¸‹æ–‡ç‰‡æ®µ")
        
        # 3. å»é‡å’Œåˆå¹¶
        deduplicated_results = self._deduplicate_results(search_results)
        print(f"ğŸ”„ å»é‡åå‰©ä½™ {len(deduplicated_results)} ä¸ªç‰‡æ®µ")
        
        # 4. æŒ‰ç›¸å…³æ€§æ’åº
        sorted_results = sorted(deduplicated_results, key=lambda x: x.relevance_score, reverse=True)
        
        # 5. é•¿åº¦æ§åˆ¶
        final_results = self._select_top_results(sorted_results, max_length)
        print(f"âœ… æœ€ç»ˆè¿”å› {len(final_results)} ä¸ªç»“æœï¼Œæ€»é•¿åº¦çº¦ {sum(len(r.text) for r in final_results)} å­—ç¬¦")
        
        return final_results
    
    def _find_all_matches(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾æ‰€æœ‰å…³é”®è¯åŒ¹é…"""
        matches = []
        
        for file_path, content in self.interview_cache.items():
            if not content.strip():
                continue
                
            for keyword in keywords:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„æœç´¢
                pattern = re.escape(keyword)
                for match in re.finditer(pattern, content, re.IGNORECASE):
                    matches.append({
                        'keyword': keyword,
                        'file_path': file_path,
                        'content': content,
                        'start': match.start(),
                        'end': match.end(),
                        'position': match.start()
                    })
        
        return matches
    
    def _extract_context(self, match: Dict[str, Any], radius: int = 200) -> Optional[SearchResult]:
        """æå–åŒ¹é…å…³é”®è¯çš„ä¸Šä¸‹æ–‡"""
        content = match['content']
        keyword = match['keyword']
        position = match['position']
        file_path = match['file_path']
        
        # è®¡ç®—ä¸Šä¸‹æ–‡è¾¹ç•Œ
        start_pos = max(0, position - radius)
        end_pos = min(len(content), position + radius)
        
        # æ‰©å±•åˆ°å¥å­è¾¹ç•Œ
        start_boundary = self._find_sentence_start(content, start_pos)
        end_boundary = self._find_sentence_end(content, end_pos)
        
        # æå–ä¸Šä¸‹æ–‡æ–‡æœ¬
        context_text = content[start_boundary:end_boundary].strip()
        
        if not context_text or len(context_text) < 10:
            return None
        
        # è·å–æ¥æºä¿¡æ¯
        metadata = get_interview_metadata(file_path)
        source_name = metadata.get('source', file_path)
        source_url = metadata.get('url')
        
        # æŸ¥æ‰¾è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­çš„æ‰€æœ‰å…³é”®è¯
        keywords_found = []
        all_keywords = match.get('all_keywords', [keyword])  # ä»å¤–éƒ¨ä¼ å…¥æ‰€æœ‰å…³é”®è¯
        
        for kw in all_keywords:
            if re.search(re.escape(kw), context_text, re.IGNORECASE):
                keywords_found.append(kw)
        
        # å¦‚æœæ²¡æœ‰ä¼ å…¥æ‰€æœ‰å…³é”®è¯ï¼Œè‡³å°‘åŒ…å«å½“å‰å…³é”®è¯
        if not keywords_found:
            keywords_found = [keyword]
        
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        relevance_score = self._calculate_relevance_score(context_text, keywords_found)
        
        # ç”Ÿæˆé«˜äº®æ–‡æœ¬
        highlighted_text = self._highlight_keywords(context_text, keywords_found)
        
        return SearchResult(
            text=context_text,
            keywords_found=keywords_found,
            relevance_score=relevance_score,
            source=source_name,
            url=source_url,
            highlighted_text=highlighted_text,
            start_position=start_boundary,
            end_position=end_boundary
        )
    
    def _find_sentence_start(self, text: str, position: int) -> int:
        """å‘å‰æŸ¥æ‰¾å¥å­å¼€å§‹ä½ç½®"""
        if position <= 0:
            return 0
        
        # å‘å‰æŸ¥æ‰¾å¥å­åˆ†éš”ç¬¦
        for i in range(position - 1, -1, -1):
            if re.match(self.sentence_delimiters, text[i]):
                return i + 1
        
        return 0
    
    def _find_sentence_end(self, text: str, position: int) -> int:
        """å‘åæŸ¥æ‰¾å¥å­ç»“æŸä½ç½®"""
        if position >= len(text):
            return len(text)
        
        # å‘åæŸ¥æ‰¾å¥å­åˆ†éš”ç¬¦
        for i in range(position, len(text)):
            if re.match(self.sentence_delimiters, text[i]):
                return i + 1
        
        return len(text)
    
    def _calculate_relevance_score(self, text: str, keywords_found: List[str]) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        if not keywords_found:
            return 0.0
        
        # åŸºç¡€åˆ†æ•°ï¼šæ¯ä¸ªå…³é”®è¯åŒ¹é… +2åˆ†
        base_score = len(keywords_found) * 2.0
        
        # å¯†åº¦å¥–åŠ±ï¼šå¤šä¸ªå…³é”®è¯åœ¨åŒä¸€æ®µè½ +0.5åˆ†/è¯
        density_bonus = 0.5 * len(keywords_found) if len(keywords_found) > 1 else 0
        
        # é•¿åº¦æƒ©ç½šï¼šè¿‡é•¿æ–‡æœ¬è½»å¾®é™åˆ†
        length_penalty = len(text) / 2000  # æ¯2000å­—ç¬¦å‡1åˆ†
        
        # å…³é”®è¯å¯†åº¦å¥–åŠ±
        total_keyword_chars = sum(len(kw) for kw in keywords_found)
        density_ratio = total_keyword_chars / len(text) if text else 0
        density_bonus += density_ratio * 5  # å¯†åº¦è¶Šé«˜åˆ†æ•°è¶Šé«˜
        
        final_score = base_score + density_bonus - length_penalty
        return max(0.1, final_score)  # æœ€ä½0.1åˆ†
    
    def _highlight_keywords(self, text: str, keywords: List[str]) -> str:
        """é«˜äº®å…³é”®è¯"""
        highlighted = text
        
        for keyword in keywords:
            # ä½¿ç”¨HTMLæ ‡è®°é«˜äº®ï¼ˆé€‚ç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
            pattern = re.escape(keyword)
            highlighted = re.sub(
                pattern, 
                f'<mark>{keyword}</mark>', 
                highlighted, 
                flags=re.IGNORECASE
            )
        
        return highlighted
    
    def _deduplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """å»é‡å’Œåˆå¹¶ç›¸ä¼¼ç»“æœ"""
        if not results:
            return []
        
        # æŒ‰æ¥æºåˆ†ç»„
        grouped_by_source = defaultdict(list)
        for result in results:
            grouped_by_source[result.source].append(result)
        
        deduplicated = []
        
        for source_results in grouped_by_source.values():
            # æŒ‰ä½ç½®æ’åº
            source_results.sort(key=lambda x: x.start_position)
            
            merged_results = []
            i = 0
            
            while i < len(source_results):
                current = source_results[i]
                j = i + 1
                
                # æŸ¥æ‰¾å¯ä»¥åˆå¹¶çš„ç›¸é‚»ç»“æœ
                while j < len(source_results):
                    next_result = source_results[j]
                    
                    # å¦‚æœé‡å æˆ–è·ç¦»å¾ˆè¿‘ï¼ˆ<100å­—ç¬¦ï¼‰ï¼Œåˆå¹¶
                    if (next_result.start_position - current.end_position) < 100:
                        # åˆå¹¶æ–‡æœ¬
                        merged_text = current.text
                        if not merged_text.endswith(next_result.text):
                            merged_text += " ... " + next_result.text
                        
                        # åˆå¹¶å…³é”®è¯
                        merged_keywords = list(set(current.keywords_found + next_result.keywords_found))
                        
                        # åˆ›å»ºåˆå¹¶åçš„ç»“æœ
                        current = SearchResult(
                            text=merged_text,
                            keywords_found=merged_keywords,
                            relevance_score=current.relevance_score + next_result.relevance_score * 0.5,
                            source=current.source,
                            url=current.url,
                            highlighted_text=self._highlight_keywords(merged_text, merged_keywords),
                            start_position=current.start_position,
                            end_position=next_result.end_position
                        )
                        j += 1
                    else:
                        break
                
                merged_results.append(current)
                i = j
            
            deduplicated.extend(merged_results)
        
        return deduplicated
    
    def _select_top_results(self, sorted_results: List[SearchResult], max_length: int) -> List[SearchResult]:
        """æ ¹æ®é•¿åº¦é™åˆ¶é€‰æ‹©é¡¶éƒ¨ç»“æœ"""
        selected = []
        total_length = 0
        
        for result in sorted_results:
            result_length = len(result.text)
            
            # å¦‚æœåŠ ä¸Šè¿™ä¸ªç»“æœä¸ä¼šè¶…è¿‡é™åˆ¶ï¼Œå°±æ·»åŠ 
            if total_length + result_length <= max_length:
                selected.append(result)
                total_length += result_length
            else:
                # å¦‚æœè¿™æ˜¯ç¬¬ä¸€ä¸ªç»“æœä¸”è¿‡é•¿ï¼Œæˆªæ–­å®ƒ
                if not selected and result_length > max_length:
                    truncated_text = result.text[:max_length - 100] + "..."
                    result.text = truncated_text
                    result.highlighted_text = self._highlight_keywords(truncated_text, result.keywords_found)
                    selected.append(result)
                break
        
        return selected


def create_content_search_service(interview_cache: Dict[str, str]) -> ContentSearchService:
    """
    åˆ›å»ºå†…å®¹æœç´¢æœåŠ¡å®ä¾‹
    
    Args:
        interview_cache: è®¿è°ˆæ–‡æœ¬ç¼“å­˜
        
    Returns:
        ContentSearchService å®ä¾‹
    """
    return ContentSearchService(interview_cache)