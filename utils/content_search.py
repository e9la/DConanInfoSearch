"""
å†…å®¹æœç´¢æœåŠ¡æ¨¡å— - åŸºäºå…³é”®è¯çš„æ™ºèƒ½è®¿è°ˆå†…å®¹æœç´¢
å®ç°ç›¸å…³æ€§è¯„åˆ†ã€ä¸Šä¸‹æ–‡æå–ã€å»é‡å’Œé•¿åº¦æ§åˆ¶
"""

import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from collections import defaultdict

from utils.interview_sources import get_interview_metadata


@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç±»"""
    text: str                      # ä¸Šä¸‹æ–‡æ–‡æœ¬
    keywords_found: List[str]      # åŒ¹é…çš„å…³é”®è¯
    relevance_score: float         # ç›¸å…³æ€§åˆ†æ•°
    source: str                    # æ¥æºæ–‡ä»¶
    url: Optional[str]             # åŸæ–‡é“¾æ¥  
    highlighted_text: str          # é«˜äº®å…³é”®è¯åçš„æ–‡æœ¬
    start_position: int            # åœ¨åŸæ–‡ä¸­çš„èµ·å§‹ä½ç½®
    end_position: int              # åœ¨åŸæ–‡ä¸­çš„ç»“æŸä½ç½®


class ContentSearchService:
    """å†…å®¹æœç´¢æœåŠ¡"""
    
    def __init__(self, interview_cache: Dict[str, str]):
        """
        åˆå§‹åŒ–æœç´¢æœåŠ¡
        
        Args:
            interview_cache: è®¿è°ˆæ–‡æœ¬ç¼“å­˜å­—å…¸
        """
        self.interview_cache = interview_cache
        self.sentence_delimiters = r'[ã€‚ï¼ï¼Ÿ\n]'
        
        # å¸¸è§å…³é”®è¯åˆ—è¡¨ï¼ˆéœ€è¦é™æƒçš„ï¼‰
        self.common_keywords = {
            # ä¸»è¦è§’è‰²ï¼ˆå‡ºç°é¢‘ç‡å¾ˆé«˜ï¼‰
            "æŸ¯å—", "å·¥è—¤æ–°ä¸€", "æ–°ä¸€", "ç°åŸå“€", "å°å“€", "å“€", "å°å…°", "æ¯›åˆ©å…°", 
            "æ¯›åˆ©å°äº”éƒ", "é˜¿ç¬ åšå£«", "åšå£«", "å°‘å¹´ä¾¦æ¢å›¢",
            # å¸¸è§è¯æ±‡
            "åä¾¦æ¢æŸ¯å—", "æŸ¯å—", "ä¾¦æ¢", "æ¡ˆä»¶", "æ¨ç†", "çœŸç›¸", "é»‘è¡£ç»„ç»‡", "ç»„ç»‡",
            "é’å±±åˆšæ˜Œ", "ä½œè€…", "æ¼«ç”»", "åŠ¨ç”»", "å‰§åœºç‰ˆ",
            # æ—¥æ–‡å¸¸è§è¯
            "ã‚³ãƒŠãƒ³", "æ–°ä¸€", "è˜­", "å“€", "ç°åŸ"
        }
        
        # è®¡ç®—å…³é”®è¯åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„é¢‘ç‡
        self._keyword_frequencies = self._calculate_keyword_frequencies()
    
    def _calculate_keyword_frequencies(self) -> Dict[str, int]:
        """è®¡ç®—å…³é”®è¯åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„å‡ºç°é¢‘ç‡"""
        frequencies = {}
        total_text = " ".join(self.interview_cache.values())
        
        for keyword in self.common_keywords:
            count = len(re.findall(re.escape(keyword), total_text, re.IGNORECASE))
            frequencies[keyword] = count
        
        return frequencies
    
    def _get_keyword_importance(self, keyword: str) -> float:
        """
        è®¡ç®—å…³é”®è¯é‡è¦æ€§åˆ†æ•°
        
        Args:
            keyword: å…³é”®è¯
            
        Returns:
            é‡è¦æ€§åˆ†æ•°ï¼Œ0.1-1.0ï¼Œè¶Šå¸¸è§åˆ†æ•°è¶Šä½
        """
        if keyword in self.common_keywords:
            # å¸¸è§å…³é”®è¯é™æƒ
            frequency = self._keyword_frequencies.get(keyword, 0)
            if frequency > 100:  # å‡ºç°è¶…è¿‡100æ¬¡çš„å…³é”®è¯å¤§å¹…é™æƒ
                return 0.2
            elif frequency > 50:  # å‡ºç°è¶…è¿‡50æ¬¡çš„å…³é”®è¯ä¸­ç­‰é™æƒ
                return 0.4
            elif frequency > 20:  # å‡ºç°è¶…è¿‡20æ¬¡çš„å…³é”®è¯è½»å¾®é™æƒ
                return 0.6
            else:
                return 0.8
        else:
            # éå¸¸è§å…³é”®è¯ä¿æŒé«˜æƒé‡
            return 1.0
    
    def _should_include_context(self, keywords_in_context: List[str], all_keywords: List[str]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥åŒ…å«æŸä¸ªä¸Šä¸‹æ–‡ç‰‡æ®µ
        
        Args:
            keywords_in_context: è¯¥ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°çš„å…³é”®è¯
            all_keywords: ç”¨æˆ·æŸ¥è¯¢çš„æ‰€æœ‰å…³é”®è¯
            
        Returns:
            æ˜¯å¦åº”è¯¥åŒ…å«è¿™ä¸ªä¸Šä¸‹æ–‡
        """
        if not keywords_in_context:
            return False
        
        # è®¡ç®—å¸¸è§å…³é”®è¯å’Œéå¸¸è§å…³é”®è¯çš„æ•°é‡
        common_count = sum(1 for kw in keywords_in_context if kw in self.common_keywords)
        uncommon_count = len(keywords_in_context) - common_count
        
        # å¦‚æœåªæœ‰å¸¸è§å…³é”®è¯ï¼Œè¦æ±‚è‡³å°‘æœ‰2ä¸ªä¸åŒçš„å…³é”®è¯
        if uncommon_count == 0:
            return len(set(keywords_in_context)) >= 2
        
        # å¦‚æœæœ‰éå¸¸è§å…³é”®è¯ï¼Œæ€»æ˜¯åŒ…å«
        return True
    
    def _calculate_context_length_limit(self, keywords_in_context: List[str], base_radius: int = 200) -> int:
        """
        æ ¹æ®å…³é”®è¯ç±»å‹åŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡é•¿åº¦
        
        Args:
            keywords_in_context: ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®è¯
            base_radius: åŸºç¡€åŠå¾„
            
        Returns:
            è°ƒæ•´åçš„ä¸Šä¸‹æ–‡åŠå¾„
        """
        if not keywords_in_context:
            return base_radius
        
        # è®¡ç®—å¹³å‡é‡è¦æ€§
        avg_importance = sum(self._get_keyword_importance(kw) for kw in keywords_in_context) / len(keywords_in_context)
        
        # å¸¸è§å…³é”®è¯å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
        if avg_importance < 0.5:
            return int(base_radius * 0.6)  # å‡å°‘40%
        elif avg_importance < 0.7:
            return int(base_radius * 0.8)  # å‡å°‘20%
        else:
            return base_radius
        
    def search_keywords(self, keywords: List[str], max_length: int = 10000) -> List[SearchResult]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢ç›¸å…³å†…å®¹
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            max_length: ç»“æœæ€»é•¿åº¦é™åˆ¶
            
        Returns:
            æŒ‰ç›¸å…³æ€§æ’åºçš„æœç´¢ç»“æœåˆ—è¡¨
        """
        if not keywords:
            return []
        
        print(f"ğŸ” å¼€å§‹æœç´¢å…³é”®è¯: {keywords}")
        
        # ğŸš€ Step 0: åˆ†æå…³é”®è¯é‡è¦æ€§
        keyword_importance = {kw: self._get_keyword_importance(kw) for kw in keywords}
        print(f"ğŸ“Š å…³é”®è¯é‡è¦æ€§: {keyword_importance}")
        
        # 1. æ”¶é›†æ‰€æœ‰åŒ¹é…ç»“æœ
        all_matches = self._find_all_matches(keywords)
        print(f"ğŸ“Š æ‰¾åˆ° {len(all_matches)} ä¸ªåˆæ­¥åŒ¹é…")
        
        # 2. æå–ä¸Šä¸‹æ–‡å¹¶è¯„åˆ†ï¼ˆåŠ å…¥è¿‡æ»¤ï¼‰
        search_results = []
        filtered_count = 0
        for match in all_matches:
            # ä¼ é€’æ‰€æœ‰å…³é”®è¯ç”¨äºä¸Šä¸‹æ–‡åˆ†æ
            match['all_keywords'] = keywords
            context = self._extract_context(match)
            if context:
                # ğŸš€ åº”ç”¨å…³é”®è¯ç»„åˆè¿‡æ»¤
                if self._should_include_context(context.keywords_found, keywords):
                    search_results.append(context)
                else:
                    filtered_count += 1
        
        print(f"ğŸ“ æå– {len(search_results)} ä¸ªä¸Šä¸‹æ–‡ç‰‡æ®µï¼ˆè¿‡æ»¤æ‰ {filtered_count} ä¸ªåªæœ‰å¸¸è§å…³é”®è¯çš„ç‰‡æ®µï¼‰")
        
        # 3. å»é‡å’Œåˆå¹¶
        deduplicated_results = self._deduplicate_results(search_results)
        print(f"ğŸ”„ å»é‡åå‰©ä½™ {len(deduplicated_results)} ä¸ªç‰‡æ®µ")
        
        # 4. æŒ‰ç›¸å…³æ€§æ’åº
        sorted_results = sorted(deduplicated_results, key=lambda x: x.relevance_score, reverse=True)
        
        # 5. é•¿åº¦æ§åˆ¶
        final_results = self._select_top_results(sorted_results, max_length)
        print(f"âœ… æœ€ç»ˆè¿”å› {len(final_results)} ä¸ªç»“æœï¼Œæ€»é•¿åº¦çº¦ {sum(len(r.text) for r in final_results)} å­—ç¬¦")
        
        return final_results
    
    def _find_all_matches(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾æ‰€æœ‰å…³é”®è¯åŒ¹é…"""
        matches = []
        
        for file_path, content in self.interview_cache.items():
            if not content.strip():
                continue
                
            for keyword in keywords:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„æœç´¢
                pattern = re.escape(keyword)
                for match in re.finditer(pattern, content, re.IGNORECASE):
                    matches.append({
                        'keyword': keyword,
                        'file_path': file_path,
                        'content': content,
                        'start': match.start(),
                        'end': match.end(),
                        'position': match.start()
                    })
        
        return matches
    
    def _extract_context(self, match: Dict[str, Any], base_radius: int = 200) -> Optional[SearchResult]:
        """æå–åŒ¹é…å…³é”®è¯çš„ä¸Šä¸‹æ–‡"""
        content = match['content']
        keyword = match['keyword']
        position = match['position']
        file_path = match['file_path']
        all_keywords = match.get('all_keywords', [keyword])
        
        # ğŸš€ é¢„å…ˆæ‰¾åˆ°è¿™ä¸ªåŒºåŸŸçš„æ‰€æœ‰å…³é”®è¯ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡é•¿åº¦
        temp_start = max(0, position - base_radius)
        temp_end = min(len(content), position + base_radius)
        temp_context = content[temp_start:temp_end]
        
        temp_keywords_found = []
        for kw in all_keywords:
            if re.search(re.escape(kw), temp_context, re.IGNORECASE):
                temp_keywords_found.append(kw)
        
        # ğŸš€ æ ¹æ®å…³é”®è¯ç±»å‹åŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡é•¿åº¦
        adjusted_radius = self._calculate_context_length_limit(temp_keywords_found, base_radius)
        
        # è®¡ç®—ä¸Šä¸‹æ–‡è¾¹ç•Œ
        start_pos = max(0, position - adjusted_radius)
        end_pos = min(len(content), position + adjusted_radius)
        
        # æ‰©å±•åˆ°å¥å­è¾¹ç•Œ
        start_boundary = self._find_sentence_start(content, start_pos)
        end_boundary = self._find_sentence_end(content, end_pos)
        
        # æå–ä¸Šä¸‹æ–‡æ–‡æœ¬
        context_text = content[start_boundary:end_boundary].strip()
        
        if not context_text or len(context_text) < 10:
            return None
        
        # è·å–æ¥æºä¿¡æ¯
        metadata = get_interview_metadata(file_path)
        source_name = metadata.get('source', file_path)
        source_url = metadata.get('url')
        
        # æŸ¥æ‰¾è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­çš„æ‰€æœ‰å…³é”®è¯
        keywords_found = []
        for kw in all_keywords:
            if re.search(re.escape(kw), context_text, re.IGNORECASE):
                keywords_found.append(kw)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³é”®è¯ï¼Œè‡³å°‘åŒ…å«å½“å‰å…³é”®è¯
        if not keywords_found:
            keywords_found = [keyword]
        
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        relevance_score = self._calculate_relevance_score(context_text, keywords_found)
        
        # ç”Ÿæˆé«˜äº®æ–‡æœ¬
        highlighted_text = self._highlight_keywords(context_text, keywords_found)
        
        return SearchResult(
            text=context_text,
            keywords_found=keywords_found,
            relevance_score=relevance_score,
            source=source_name,
            url=source_url,
            highlighted_text=highlighted_text,
            start_position=start_boundary,
            end_position=end_boundary
        )
    
    def _find_sentence_start(self, text: str, position: int) -> int:
        """å‘å‰æŸ¥æ‰¾å¥å­å¼€å§‹ä½ç½®"""
        if position <= 0:
            return 0
        
        # å‘å‰æŸ¥æ‰¾å¥å­åˆ†éš”ç¬¦
        for i in range(position - 1, -1, -1):
            if re.match(self.sentence_delimiters, text[i]):
                return i + 1
        
        return 0
    
    def _find_sentence_end(self, text: str, position: int) -> int:
        """å‘åæŸ¥æ‰¾å¥å­ç»“æŸä½ç½®"""
        if position >= len(text):
            return len(text)
        
        # å‘åæŸ¥æ‰¾å¥å­åˆ†éš”ç¬¦
        for i in range(position, len(text)):
            if re.match(self.sentence_delimiters, text[i]):
                return i + 1
        
        return len(text)
    
    def _calculate_relevance_score(self, text: str, keywords_found: List[str]) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        if not keywords_found:
            return 0.0
        
        # ğŸš€ è®¡ç®—åŸºäºé‡è¦æ€§çš„åŠ æƒåˆ†æ•°
        importance_weighted_score = 0.0
        for keyword in keywords_found:
            importance = self._get_keyword_importance(keyword)
            importance_weighted_score += importance * 2.0  # æ¯ä¸ªå…³é”®è¯åŸºç¡€åˆ†2åˆ†ï¼ŒæŒ‰é‡è¦æ€§åŠ æƒ
        
        # å¯†åº¦å¥–åŠ±ï¼šå¤šä¸ªå…³é”®è¯åœ¨åŒä¸€æ®µè½ï¼ˆä½†è€ƒè™‘é‡è¦æ€§ï¼‰
        if len(keywords_found) > 1:
            # è®¡ç®—å¹³å‡é‡è¦æ€§
            avg_importance = sum(self._get_keyword_importance(kw) for kw in keywords_found) / len(keywords_found)
            density_bonus = 0.5 * len(keywords_found) * avg_importance
        else:
            density_bonus = 0
        
        # é•¿åº¦æƒ©ç½šï¼šè¿‡é•¿æ–‡æœ¬è½»å¾®é™åˆ†
        length_penalty = len(text) / 2000  # æ¯2000å­—ç¬¦å‡1åˆ†
        
        # å…³é”®è¯å¯†åº¦å¥–åŠ±ï¼ˆä½†é™ä½å¸¸è§è¯çš„å½±å“ï¼‰
        weighted_keyword_chars = sum(len(kw) * self._get_keyword_importance(kw) for kw in keywords_found)
        density_ratio = weighted_keyword_chars / len(text) if text else 0
        density_bonus += density_ratio * 3  # è°ƒæ•´ç³»æ•°ä»5é™åˆ°3
        
        final_score = importance_weighted_score + density_bonus - length_penalty
        return max(0.1, final_score)  # æœ€ä½0.1åˆ†
    
    def _highlight_keywords(self, text: str, keywords: List[str]) -> str:
        """é«˜äº®å…³é”®è¯"""
        highlighted = text
        
        for keyword in keywords:
            # ä½¿ç”¨HTMLæ ‡è®°é«˜äº®ï¼ˆé€‚ç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
            pattern = re.escape(keyword)
            highlighted = re.sub(
                pattern, 
                f'<mark>{keyword}</mark>', 
                highlighted, 
                flags=re.IGNORECASE
            )
        
        return highlighted
    
    def _deduplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """å»é‡å’Œåˆå¹¶ç›¸ä¼¼ç»“æœ"""
        if not results:
            return []
        
        # æŒ‰æ¥æºåˆ†ç»„
        grouped_by_source = defaultdict(list)
        for result in results:
            grouped_by_source[result.source].append(result)
        
        deduplicated = []
        
        for source_results in grouped_by_source.values():
            # æŒ‰ä½ç½®æ’åº
            source_results.sort(key=lambda x: x.start_position)
            
            merged_results = []
            i = 0
            
            while i < len(source_results):
                current = source_results[i]
                j = i + 1
                
                # æŸ¥æ‰¾å¯ä»¥åˆå¹¶çš„ç›¸é‚»ç»“æœ
                while j < len(source_results):
                    next_result = source_results[j]
                    
                    # å¦‚æœé‡å æˆ–è·ç¦»å¾ˆè¿‘ï¼ˆ<100å­—ç¬¦ï¼‰ï¼Œåˆå¹¶
                    if (next_result.start_position - current.end_position) < 100:
                        # åˆå¹¶æ–‡æœ¬
                        merged_text = current.text
                        if not merged_text.endswith(next_result.text):
                            merged_text += " ... " + next_result.text
                        
                        # åˆå¹¶å…³é”®è¯
                        merged_keywords = list(set(current.keywords_found + next_result.keywords_found))
                        
                        # åˆ›å»ºåˆå¹¶åçš„ç»“æœ
                        current = SearchResult(
                            text=merged_text,
                            keywords_found=merged_keywords,
                            relevance_score=current.relevance_score + next_result.relevance_score * 0.5,
                            source=current.source,
                            url=current.url,
                            highlighted_text=self._highlight_keywords(merged_text, merged_keywords),
                            start_position=current.start_position,
                            end_position=next_result.end_position
                        )
                        j += 1
                    else:
                        break
                
                merged_results.append(current)
                i = j
            
            deduplicated.extend(merged_results)
        
        return deduplicated
    
    def _select_top_results(self, sorted_results: List[SearchResult], max_length: int) -> List[SearchResult]:
        """æ ¹æ®é•¿åº¦é™åˆ¶é€‰æ‹©é¡¶éƒ¨ç»“æœ"""
        selected = []
        total_length = 0
        
        for result in sorted_results:
            result_length = len(result.text)
            
            # å¦‚æœåŠ ä¸Šè¿™ä¸ªç»“æœä¸ä¼šè¶…è¿‡é™åˆ¶ï¼Œå°±æ·»åŠ 
            if total_length + result_length <= max_length:
                selected.append(result)
                total_length += result_length
            else:
                # å¦‚æœè¿™æ˜¯ç¬¬ä¸€ä¸ªç»“æœä¸”è¿‡é•¿ï¼Œæˆªæ–­å®ƒ
                if not selected and result_length > max_length:
                    truncated_text = result.text[:max_length - 100] + "..."
                    result.text = truncated_text
                    result.highlighted_text = self._highlight_keywords(truncated_text, result.keywords_found)
                    selected.append(result)
                break
        
        return selected


def create_content_search_service(interview_cache: Dict[str, str]) -> ContentSearchService:
    """
    åˆ›å»ºå†…å®¹æœç´¢æœåŠ¡å®ä¾‹
    
    Args:
        interview_cache: è®¿è°ˆæ–‡æœ¬ç¼“å­˜
        
    Returns:
        ContentSearchService å®ä¾‹
    """
    return ContentSearchService(interview_cache)